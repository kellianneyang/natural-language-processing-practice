{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e2a63a",
   "metadata": {},
   "source": [
    "# Authorship Attribution\n",
    "\n",
    "From Chapter 5 of Getting Started with Natural Language Processing (2022, Kochmar)\n",
    "\n",
    "The goal of this notebook is to train and evaluate a machine learning algorithm to classify a sentence as written by one of two authors (classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576b4c7",
   "metadata": {},
   "source": [
    "# 1. Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f8b432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\yang0108\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yang0108\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg # data for training\n",
    "nltk.download('punkt') # sentence tokenizer\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1e7bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select data for training\n",
    "\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b77666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author 1 training set length: 11464 sentences\n",
      "Author 1 testing set length: 4999 sentences\n",
      "Author 2 training set length: 5269 sentences\n",
      "Author 2 testing set length: 1907 sentences\n"
     ]
    }
   ],
   "source": [
    "# shakespeare and austen each have three attributed works, so we will use\n",
    "# them as our data\n",
    "\n",
    "# will use 2 works from each author as training and pretest data; third work\n",
    "# will be reserved for testing data\n",
    "\n",
    "# create training and testing data sentences for author1 (austen)\n",
    "author1_train = gutenberg.sents('austen-emma.txt') + \\\n",
    "                gutenberg.sents('austen-persuasion.txt')\n",
    "author1_test = gutenberg.sents('austen-sense.txt')\n",
    "\n",
    "# create training and testing data sentences for author1 (shakespeare)\n",
    "author2_train = gutenberg.sents('shakespeare-caesar.txt') + \\\n",
    "                gutenberg.sents('shakespeare-hamlet.txt')\n",
    "author2_test = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "\n",
    "# check length of data sets\n",
    "print(f\"Author 1 training set length: {len(author1_train)} sentences\")\n",
    "print(f\"Author 1 testing set length: {len(author1_test)} sentences\")\n",
    "print(f\"Author 2 training set length: {len(author2_train)} sentences\")\n",
    "print(f\"Author 2 testing set length: {len(author2_test)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a058d",
   "metadata": {},
   "source": [
    "# 2. Statistics comparing authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b62a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function to calculate statistics\n",
    "def statistics(gutenberg_data):\n",
    "    \n",
    "    # iterate over works given\n",
    "    for work in gutenberg_data:\n",
    "        \n",
    "        # number of characters in a work\n",
    "        num_chars = len(gutenberg.raw(work))\n",
    "        \n",
    "        # number of words in a work\n",
    "        num_words = len(gutenberg.words(work))\n",
    "        \n",
    "        # number of sentences in a work\n",
    "        num_sents = len(gutenberg.sents(work))\n",
    "        \n",
    "        # number of unique words in a work (python set over list of all words)\n",
    "        num_vocab = len(set(w.lower() for w in gutenberg.words(work)))\n",
    "        \n",
    "        print(\n",
    "            # average length of words\n",
    "            round(num_chars/num_words),\n",
    "            \n",
    "            # average length of sentences\n",
    "            round(num_words/num_sents),\n",
    "            \n",
    "            # average number of times each word is used in a text by the author\n",
    "            round(num_words/num_vocab),\n",
    "            \n",
    "            # name of work\n",
    "            work\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1cd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of works to get statistics for\n",
    "gutenberg_data = ['austen-emma.txt', 'austen-persuasion.txt', \n",
    "                  'austen-sense.txt', 'shakespeare-caesar.txt', \n",
    "                  'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab46cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n"
     ]
    }
   ],
   "source": [
    "# run statistics function on list of works\n",
    "statistics(gutenberg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889fd28",
   "metadata": {},
   "source": [
    "Austen on average uses slightly longer words than Shakespeare (an average of 5 letters to 4). She also uses much longer sentences (an average of 25-28 words, depending on the text, to 12). Perhaps influenced by the length of her works, the average number of times a single word is used across a whole text by Austen ranges from 17-26, while Shakespeare's words only get repeated an average of 7-9 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab6855",
   "metadata": {},
   "source": [
    "# 3. Split data into training and pretesting sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "497c56d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 16733 sentences\n"
     ]
    }
   ],
   "source": [
    "# add all sentences in training set to list, keeping author label\n",
    "all_sents = [(sent, \"austen\") for sent in author1_train]\n",
    "all_sents += [(sent, \"shakespeare\") for sent in author2_train]\n",
    "\n",
    "# check number of sentences in all_sents\n",
    "print(f\"Training set length: {str(len(all_sents))} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3fac21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep set of labels (authors) as values\n",
    "values = [author for (sent, author) in all_sents]\n",
    "\n",
    "# instantiate stratified shuffle split\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# create empty lists for train and pretest sets\n",
    "strat_train_set = []\n",
    "strat_pretest_set = []\n",
    "\n",
    "# perform split\n",
    "for train_index, pretest_index in split.split(all_sents, values):\n",
    "    strat_train_set = [all_sents[index] for index in train_index]\n",
    "    strat_pretest_set = [all_sents[index] for index in pretest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557b929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test_set data to same structure as strat_train_set and strat_pretest_set\n",
    "test_set = [(sent, \"austen\") for sent in author1_test]\n",
    "test_set += [(sent, \"shakespeare\") for sent in author2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39debb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Category     (strat_train_set and strat_pretest_set)  strat_train_set  strat_pretest_set  test_set  \n",
      " austen        0.685113                                 0.685119         0.685091           0.723863 \n",
      " shakespeare   0.314887                                 0.314881         0.314909           0.276137 \n"
     ]
    }
   ],
   "source": [
    "# check the proportions of the data in sets: \n",
    "# overall (strat_train and strat_pretest), strat_train_set, strat_pretest_set, and test_set\n",
    "\n",
    "# define helper function to calculate proportions\n",
    "def cat_proportions(data, cat):\n",
    "    \n",
    "    # initialize count of sentences\n",
    "    count = 0\n",
    "    \n",
    "    # iterate over sentences in data\n",
    "    for item in data:\n",
    "        \n",
    "        # if the item's author is cat\n",
    "        if item[1] == cat:\n",
    "            \n",
    "            # increment count\n",
    "            count += 1\n",
    "    \n",
    "    # return proportion\n",
    "    return float(count) / float(len(data))\n",
    "\n",
    "# set authors as categories\n",
    "categories = [\"austen\", \"shakespeare\"]\n",
    "\n",
    "# create rows for table\n",
    "rows = []\n",
    "rows.append([\"Category\", \"(strat_train_set and strat_pretest_set)\", \n",
    "             \"strat_train_set\", \"strat_pretest_set\", \"test_set\"])\n",
    "\n",
    "# populate list for rows\n",
    "for cat in categories:\n",
    "    rows.append([cat, \n",
    "                 f\"{cat_proportions(all_sents, cat): .6f}\",\n",
    "                 f\"{cat_proportions(strat_train_set, cat): .6f}\",\n",
    "                 f\"{cat_proportions(strat_pretest_set, cat): .6f}\",\n",
    "                 f\"{cat_proportions(test_set, cat): .6f}\"])\n",
    "\n",
    "# make table, adjust column widths, and populate rows\n",
    "columns = zip(*rows)\n",
    "column_widths = [max(len(item) for item in col) for col in columns]\n",
    "for row in rows:\n",
    "    print(''.join(' {:{width}} '.format(row[i], width = column_widths[i])\n",
    "                 for i in range(0, len(row))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679c2ee",
   "metadata": {},
   "source": [
    "# 4. Extract words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a81ed7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13386\n",
      "3347\n"
     ]
    }
   ],
   "source": [
    "# create function\n",
    "def get_features(text):\n",
    "    \n",
    "    # create empty dictionary\n",
    "    features = {}\n",
    "    \n",
    "    # create word list from text\n",
    "    word_list = [word for word in text]\n",
    "    \n",
    "    # iterate over word_list\n",
    "    for word in word_list:\n",
    "        \n",
    "        # set a presence flag to \"True\" in dictionary\n",
    "        features[word] = True\n",
    "    \n",
    "    # return dictionary of features\n",
    "    return features\n",
    "\n",
    "# get feature dictionary for strat_train_set and strat_pretest_set\n",
    "train_features = [(get_features(sents), label) for (sents, label) in strat_train_set]\n",
    "pretest_features = [(get_features(sents), label) for (sents, label) in strat_pretest_set]\n",
    "\n",
    "# checks\n",
    "print(len(train_features))\n",
    "print(len(pretest_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223cb2f",
   "metadata": {},
   "source": [
    "# 5. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0caa14",
   "metadata": {},
   "source": [
    "## 5.1 Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88ce4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
