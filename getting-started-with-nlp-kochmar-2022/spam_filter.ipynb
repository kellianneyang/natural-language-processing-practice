{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23619c3",
   "metadata": {},
   "source": [
    "# Spam Filter (NLP Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1bf8fd",
   "metadata": {},
   "source": [
    "From Chapter 2 of Getting Started with Natural Language Processing (2022, Kochmar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b58ab",
   "metadata": {},
   "source": [
    "# Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8813890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yang0108\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "# general\n",
    "import os # to iterate through folders\n",
    "import codecs # helps with different text encodings\n",
    "import random # to shuffle the order of emails (to prep for selecting train and test sets)\n",
    "\n",
    "# processing\n",
    "import nltk # nlp toolkit\n",
    "from nltk import word_tokenize # nltk's word tokenizer\n",
    "nltk.download('punkt') # nltk's sentence tokenizer\n",
    "\n",
    "# modeling\n",
    "from nltk import NaiveBayesClassifier, classify # NLTK's Naive Bayes Classifier\n",
    "from nltk.text import Text # NLTK's Text data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63291653",
   "metadata": {},
   "source": [
    "## Step 1: Define data and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65cb17",
   "metadata": {},
   "source": [
    "Read in the data to train and test the spam filter. Shuffle the data to prep it for splitting into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53782937",
   "metadata": {},
   "source": [
    "Data: \n",
    "Enron email dataset (www.cs.cmu.edu/~enron/)\n",
    "\n",
    "Download subsets at: http://mng.bz/WxYg\n",
    "\n",
    "Subset and data collection processes described: http://www2.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe84b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the contents of the ham files\n",
    "\n",
    "# initialize empty ham_list\n",
    "ham_list = []\n",
    "\n",
    "# use os.walk to access files that have been uploaded to drive\n",
    "for root, dirs, files in os.walk(\"Data/enron1/ham\"):\n",
    "    \n",
    "    # iterate through each separate file\n",
    "    for file in files:\n",
    "\n",
    "        # if the file is a text file\n",
    "        if file.endswith('.txt'):\n",
    "\n",
    "            # open the file as f\n",
    "            with open(os.path.join(root, file), 'r', \n",
    "                      encoding = \"ISO-8859-1\", \n",
    "                      errors=\"ignore\") as f:\n",
    "                \n",
    "                # read the file and save as variable text\n",
    "                text = f.read()\n",
    "\n",
    "                # append text to ham_list\n",
    "                ham_list.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403326fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat step above for spam files\n",
    "\n",
    "spam_list = []\n",
    "for root, dirs, files in os.walk(\"Data/enron1/spam\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(root, file), 'r', \n",
    "                      encoding = \"ISO-8859-1\", \n",
    "                      errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                spam_list.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070c0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of spam_list: 1500 \n",
      "\n",
      "Length of ham_list:       3672\n",
      "\n",
      "First example of spam: \n",
      "Subject: dobmeos with hgh my energy level has gone up! Stukm\n",
      "Introducing\n",
      "Doctor - formulated\n",
      "Hgh\n",
      "Human growth hormone - also called hgh\n",
      "Is referred to in medical science as the master hormone. It is very plentiful\n",
      "When we are young, but near the age of twenty - one our bodies begin to produce\n",
      "Less of it. By the time we are forty nearly everyone is deficient in hgh,\n",
      "And at eighty our production has normally diminished at least 90 - 95%.\n",
      "Advantages of hgh:\n",
      "- increased muscle strength\n",
      "- loss in body fat\n",
      "- increased bone density\n",
      "- lower blood pressure\n",
      "- quickens wound healing\n",
      "- reduces cellulite\n",
      "- improved vision\n",
      "- wrinkle disappearance\n",
      "- increased skin thickness texture\n",
      "- increased energy levels\n",
      "- improved sleep and emotional stability\n",
      "- improved memory and mental alertness\n",
      "- increased sexual potency\n",
      "- resistance to common illness\n",
      "- strengthened heart muscle\n",
      "- controlled cholesterol\n",
      "- controlled mood swings\n",
      "- new hair growth and color restore\n",
      "Read\n",
      "More at this website\n",
      "Unsubscribe\n",
      " \n",
      "\n",
      "First example of ham: \n",
      "       Subject: christmas tree farm pictures\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that files have been uploaded correctly\n",
    "\n",
    "# print the length of spam_list and ham_list\n",
    "# spam_list should contain 1,500 and ham_list should contain 3,672\n",
    "print(f\"Length of spam_list: {len(spam_list)} \\n\\nLength of ham_list: \\\n",
    "      {len(ham_list)}\") \n",
    "\n",
    "# print extra lines for readability\n",
    "print()\n",
    "\n",
    "# print the first example of both lists to verify contents\n",
    "print(f\"First example of spam: \\n{spam_list[0]} \\n\\nFirst example of ham: \\n \\\n",
    "      {ham_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b874bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read both list contents into single list all_emails, storing email content \n",
    "# and content label (ham or spam) in tuples\n",
    "\n",
    "# create all_emails with spam_list\n",
    "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
    "\n",
    "# add ham_list to all_emails\n",
    "all_emails += [(email_content, \"ham\") for email_content in ham_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d6bec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the seed of the random operator to make sure that all future runs \n",
    "# will shuffle the data in the same way\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf3a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the list to prepare for splitting into train and test sets\n",
    "random.shuffle(all_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123bfacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 5172 emails\n"
     ]
    }
   ],
   "source": [
    "# check list size\n",
    "print(f\"Dataset size = {len(all_emails)} emails\") \n",
    "# 5,172 is the correct number (1,500 spam + 3,672 ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1e607",
   "metadata": {},
   "source": [
    "# Step 2: Split text into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493dbe7",
   "metadata": {},
   "source": [
    "Use NLTK's word tokenizer to split the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7994613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenize function\n",
    "def tokenize(text):\n",
    "\n",
    "  # create word_list that contains all lowercase tokenized words from input\n",
    "  word_list = [word for word in word_tokenize(text)]\n",
    "\n",
    "  # return word_list\n",
    "  return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1fbe8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words', '?']\n"
     ]
    }
   ],
   "source": [
    "# test tokenize function\n",
    "\n",
    "# give it some text as input\n",
    "input = \"What's the best way to split a sentence into words?\"\n",
    "\n",
    "# print results of tokenize function\n",
    "print(tokenize(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6cd60",
   "metadata": {},
   "source": [
    "# Step 3: Extract and normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22dc207",
   "metadata": {},
   "source": [
    "This step uses the functionality built in the previous step for adding tokenized words into word_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e02208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define get_features function to extract features from a text\n",
    "# this function takes in a text and returns a dictionary\n",
    "# keys in the dictionary are the words in the text\n",
    "# values in the dictionary are True\n",
    "def get_features(text):\n",
    "\n",
    "  # initialize features dictionary\n",
    "  features = {}\n",
    "\n",
    "  # create word_list, which is a list of all tokenized words in the text\n",
    "  # copy the tokenize functionality from the tokenize function above\n",
    "  # add in normalization of characters into lowercase\n",
    "  word_list = [word for word in word_tokenize(text.lower())]\n",
    "\n",
    "  # for each word in word_list\n",
    "  for word in word_list:\n",
    "    \n",
    "    # switch on the \"flag\" that this word is contained in this text (email)\n",
    "    features[word] = True\n",
    "  \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f017ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'participate': True, 'in': True, 'our': True, 'new': True, 'lottery': True, 'now': True, '!': True}\n"
     ]
    }
   ],
   "source": [
    "# check the get_features function\n",
    "\n",
    "# give get_features some text\n",
    "print(get_features(\"Participate In Our New Lottery NOW!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bc1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples called all_features\n",
    "# that iterates over all texts (emails) in the all_emails list\n",
    "# and adds a tuple for each email\n",
    "# the tuple contains the features dictionary (from get_features) and the \n",
    "# label for each email (ham or spam) (from all_emails)\n",
    "\n",
    "all_features = [(get_features(email), label) for (email, label) in all_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b810d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_features length: 5172 \n",
      "all_features type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# check the length and type of all_features (length should be same as number \n",
    "# of emails, 5,172; type should be list)\n",
    "print(f\"all_features length: {len(all_features)} \\nall_features type: \\\n",
    "{type(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f797ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'subject': True, ':': True, 'bloodline': True, ',': True, 'ahead': True, 'of': True, 'the': True, 'street': True, 'microcap': True, 'alert': True, 'when': True, 'living': True, 'with': True, 'sheriff': True, 'is': True, 'obsequious': True, 'blood': True, 'clot': True, 'beyond': True, 'deficit': True, 'reach': True, 'an': True, 'understanding': True, 'toward': True, '.': True, '[': True, '3': True}, 'spam')\n",
      "\n",
      "({'subject': True, ':': True, 'mobil': True, 'beaumont': True, '-': True, 'marol': True, 'rebecca': True, 'for': True, 'in': True, 'march': True, ',': True, 'beginning': True, 'on': True, '21': True, 'hpl': True, 'started': True, 'delivering': True, '30': True, '000/d': True, 'midcon': True, '(': True, 'just': True, 'like': True, 'we': True, 'did': True, 'dec': True, 'and': True, 'jan': True, 'maybe': True, 'feb': True, 'too': True, ')': True, 'check': True, 'with': True, 'daren': True, 'farmer': True, 'when': True, 'you': True, 'get': True, 'ready': True, 'to': True, 'do': True, 'the': True, 'flash': True, 'invoice': True, 'first': True, 'confirm': True, 'volume': True, 'amounts': True, 'dates': True, 'thanks': True, '!': True, 'lee': True}, 'ham')\n",
      "\n",
      "{'subject': True, ':': True, 'bloodline': True, ',': True, 'ahead': True, 'of': True, 'the': True, 'street': True, 'microcap': True, 'alert': True, 'when': True, 'living': True, 'with': True, 'sheriff': True, 'is': True, 'obsequious': True, 'blood': True, 'clot': True, 'beyond': True, 'deficit': True, 'reach': True, 'an': True, 'understanding': True, 'toward': True, '.': True, '[': True, '3': True}\n",
      "\n",
      "{'subject': True, ':': True, 'mobil': True, 'beaumont': True, '-': True, 'marol': True, 'rebecca': True, 'for': True, 'in': True, 'march': True, ',': True, 'beginning': True, 'on': True, '21': True, 'hpl': True, 'started': True, 'delivering': True, '30': True, '000/d': True, 'midcon': True, '(': True, 'just': True, 'like': True, 'we': True, 'did': True, 'dec': True, 'and': True, 'jan': True, 'maybe': True, 'feb': True, 'too': True, ')': True, 'check': True, 'with': True, 'daren': True, 'farmer': True, 'when': True, 'you': True, 'get': True, 'ready': True, 'to': True, 'do': True, 'the': True, 'flash': True, 'invoice': True, 'first': True, 'confirm': True, 'volume': True, 'amounts': True, 'dates': True, 'thanks': True, '!': True, 'lee': True}\n",
      "\n",
      "spam\n",
      "\n",
      "ham\n",
      "\n",
      "27\n",
      "\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# check some items in all_features to verify contents\n",
    "print(all_features[0]) \n",
    "# should return tuple with dictionary and label of first item\n",
    "\n",
    "print()\n",
    "print(all_features[99]) \n",
    "# should return tuple with dictionary and label of hundredth item\n",
    "\n",
    "print()\n",
    "print(all_features[0][0]) \n",
    "# should return feature dictionary of first item\n",
    "\n",
    "print()\n",
    "print(all_features[99][0]) \n",
    "# should return feature dictionary of hundredth item\n",
    "\n",
    "print()\n",
    "print(all_features[0][1]) \n",
    "# should return label of first item\n",
    "\n",
    "print()\n",
    "print(all_features[99][1]) \n",
    "# should return label of hundredth item\n",
    "\n",
    "print()\n",
    "print(len(all_features[0][0])) \n",
    "# should return length of feature dictionary of first item (number of unique \n",
    "# words in email)\n",
    "\n",
    "print()\n",
    "print(len(all_features[99][0])) \n",
    "# should return length of feature dictionary of hundredth item (number of \n",
    "# unique words in email)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e81966f",
   "metadata": {},
   "source": [
    "# Step 4: Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a28fd8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, proportion):\n",
    "\n",
    "    # get the size of the training set based on the proportion\n",
    "    train_size = int(len(features) * proportion)\n",
    "\n",
    "    # select out the number of items you need for training from the features \n",
    "    # dictionary\n",
    "    train_set = features[:train_size]\n",
    "\n",
    "    # select out the rest of the items you need for testing from the features \n",
    "    # dictionary\n",
    "    test_set = features[train_size:]\n",
    "\n",
    "    # make sure the data split correctly, print the number of training and \n",
    "    # test items\n",
    "    print(f\"Training set size: {len(train_set)} emails\")\n",
    "    print(f\"Test set size: {len(test_set)} emails\")\n",
    "\n",
    "    # initialize the classifier\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    # return\n",
    "    return train_set, test_set, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "325c11ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4137 emails\n",
      "Test set size: 1035 emails\n"
     ]
    }
   ],
   "source": [
    "# apply the train function using 80% of emails for training\n",
    "train_set, test_set, classifier = train(all_features, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8113f1",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb6d64",
   "metadata": {},
   "source": [
    "NLTK's classifier returns an accuracy score for the train and test sets. It also allows you to inspect the more informative features (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "811b5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluate function to test accuracy of classifier\n",
    "def evaluate(train_set, test_set, classifier):\n",
    "\n",
    "    # accuracy on training set\n",
    "    print(f\"Accuracy on the training set: \\\n",
    "{classify.accuracy(classifier, train_set)}\")\n",
    "  \n",
    "    # accuracy on test set\n",
    "    print(f\"Accuracy on the test set: \\\n",
    "{classify.accuracy(classifier, test_set)}\")\n",
    "  \n",
    "    # select top 50 most informative features to show\n",
    "    print(classifier.show_most_informative_features(50)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a371f057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.9608411892675852\n",
      "Accuracy on the test set: 0.9420289855072463\n",
      "Most Informative Features\n",
      "               forwarded = True              ham : spam   =    198.3 : 1.0\n",
      "                    2004 = True             spam : ham    =    143.8 : 1.0\n",
      "                     nom = True              ham : spam   =    126.0 : 1.0\n",
      "            prescription = True             spam : ham    =    122.9 : 1.0\n",
      "                    pain = True             spam : ham    =     98.8 : 1.0\n",
      "                  health = True             spam : ham    =     82.7 : 1.0\n",
      "                     ect = True              ham : spam   =     76.8 : 1.0\n",
      "                    2001 = True              ham : spam   =     75.8 : 1.0\n",
      "                featured = True             spam : ham    =     74.7 : 1.0\n",
      "              nomination = True              ham : spam   =     72.1 : 1.0\n",
      "             medications = True             spam : ham    =     69.9 : 1.0\n",
      "                  differ = True             spam : ham    =     66.7 : 1.0\n",
      "                creative = True             spam : ham    =     65.1 : 1.0\n",
      "             subscribers = True             spam : ham    =     65.1 : 1.0\n",
      "                   risks = True             spam : ham    =     63.4 : 1.0\n",
      "                     pro = True             spam : ham    =     60.2 : 1.0\n",
      "                  shares = True             spam : ham    =     58.6 : 1.0\n",
      "                   cheap = True             spam : ham    =     55.4 : 1.0\n",
      "                       u = True             spam : ham    =     55.4 : 1.0\n",
      "                    2005 = True             spam : ham    =     54.5 : 1.0\n",
      "                     ali = True             spam : ham    =     53.8 : 1.0\n",
      "                inherent = True             spam : ham    =     52.2 : 1.0\n",
      "            solicitation = True             spam : ham    =     52.2 : 1.0\n",
      "                deciding = True             spam : ham    =     50.6 : 1.0\n",
      "                   epson = True             spam : ham    =     50.6 : 1.0\n",
      "                    sony = True             spam : ham    =     50.6 : 1.0\n",
      "                  stocks = True             spam : ham    =     50.6 : 1.0\n",
      "                      cc = True              ham : spam   =     49.9 : 1.0\n",
      "                 advises = True             spam : ham    =     45.8 : 1.0\n",
      "                   cisco = True             spam : ham    =     45.8 : 1.0\n",
      "                thousand = True             spam : ham    =     44.8 : 1.0\n",
      "                 foresee = True             spam : ham    =     44.2 : 1.0\n",
      "                mailings = True             spam : ham    =     44.2 : 1.0\n",
      "                   adobe = True             spam : ham    =     40.0 : 1.0\n",
      "                   susan = True              ham : spam   =     39.7 : 1.0\n",
      "                      ex = True             spam : ham    =     38.1 : 1.0\n",
      "                   women = True             spam : ham    =     38.1 : 1.0\n",
      "                 beliefs = True             spam : ham    =     37.7 : 1.0\n",
      "                  proven = True             spam : ham    =     37.7 : 1.0\n",
      "                     bob = True              ham : spam   =     37.5 : 1.0\n",
      "           advertisement = True             spam : ham    =     36.1 : 1.0\n",
      "                 explode = True             spam : ham    =     36.1 : 1.0\n",
      "                 foreign = True             spam : ham    =     36.1 : 1.0\n",
      "                powerful = True             spam : ham    =     36.1 : 1.0\n",
      "                   steve = True              ham : spam   =     35.8 : 1.0\n",
      "                     713 = True              ham : spam   =     35.6 : 1.0\n",
      "                    duke = True              ham : spam   =     35.0 : 1.0\n",
      "                 comfort = True             spam : ham    =     34.5 : 1.0\n",
      "                     fat = True             spam : ham    =     34.5 : 1.0\n",
      "              medication = True             spam : ham    =     34.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# use evaluate function on classifier\n",
    "evaluate(train_set, test_set, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd4f77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCKS in HAM:\n",
      "Displaying 1 of 1 matches:\n",
      "ad my portfolio is diversified into stocks that have lost even more money than\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "\n",
      "STOCKS in SPAM:\n",
      "Displaying 2 of 2 matches:\n",
      "ims and do your own due diligence . stocks to play ( s 2 p ) profiles are not \n",
      "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
      "Displaying 1 of 1 matches:\n",
      "cautions that small and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 1 of 1 matches:\n",
      "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
      "Displaying 3 of 3 matches:\n",
      "ancements but may be one of the few stocks left in this industry group that is\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "in apple investments , inc profiled stocks . in order to be in full compliance\n",
      "Displaying 1 of 1 matches:\n",
      "subject : fwd : screw doctors . stocks available . vlagr @ . x _ a _ nax .\n",
      "Displaying 1 of 1 matches:\n",
      "cautions that small and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 4 of 4 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "eep in mind that when trading small stocks like the company above there is a c\n",
      "t professional before investing any stocks or mutual funds .\n",
      "Displaying 4 of 4 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "eep in mind that when trading small stocks like the company above there is a c\n",
      "t professional before investing any stocks or mutual funds .\n",
      "Displaying 1 of 1 matches:\n",
      "ecializing in undervalued small cap stocks for immediate breakout erhc and exx\n",
      "Displaying 6 of 6 matches:\n",
      " if you knew about these low priced stocks : otcbb : zapz : closed march 31 st\n",
      " following points : * many of these stocks are undiscovered and uncovered ! wh\n",
      " ! ! * * many of these undiscovered stocks are like coiled springs , wound tig\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "fessionally not multi - level - not stocks - not real estate no cost tele - se\n",
      "Displaying 2 of 2 matches:\n",
      "ng their gains . select gold mining stocks are the hot flyers of the otc . his\n",
      "is letter cautions that micro - cap stocks are high - risk investments and tha\n",
      "Displaying 6 of 6 matches:\n",
      "hem : ( big money was made in these stocks by savvy investors who timed them r\n",
      "g filthy , stinking ri ' ch in tiny stocks no one has ever heard of until now \n",
      "ynamic things . some of these small stocks have absolutely exploded in price r\n",
      "'' occur . as with many micro - cap stocks , today ' s company has additional \n",
      " ema - il pertaining to investing , stocks or securities must be understood as\n",
      "ntative before deciding to trade in stocks featured within this ema - il . non\n",
      "Displaying 4 of 4 matches:\n",
      "hree days . play of the week tracks stocks on downward trends , foresees botto\n",
      "mark is our uncanny ability to spot stocks that have bottomed - out and antici\n",
      "ound and upward trend . most of the stocks we track rebound and peak within ju\n",
      "om third party . investing in penny stocks is high risk and you should seek pr\n",
      "Displaying 2 of 2 matches:\n",
      "rt identifying defense and security stocks ready to explode look at the moves \n",
      " actual exchanges where small - cap stocks are traded . silica stopband doorkn\n",
      "Displaying 4 of 4 matches:\n",
      "y agree , some , not all , of these stocks move in price because they are prom\n",
      "tands or that as with many microcap stocks , today ' s company has additional \n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 5 of 5 matches:\n",
      "5 where were you when the following stocks exploded : scos : exploded from . 3\n",
      "d . 80 on friday . face it . little stocks can mean big gains for you . this r\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "blication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "blication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 3 of 3 matches:\n",
      "torage inc. play of the week tracks stocks on downward trends , foresees botto\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      "nt opportunity drummond , small cap stocks alert newsletter must read - alert \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      " lose money from investing in penny stocks . - - - - - - - - - - - - - - - - -\n",
      "Displaying 4 of 4 matches:\n",
      "n this stock . some of these smal | stocks are absoiuteiy fiying , as many of \n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "biication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 3 of 3 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      " lose money from investing in penny stocks . if you wish to stop future mailin\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 1 of 1 matches:\n",
      " receive first notice on run - away stocks traders ' monthly alert january pic\n",
      "Displaying 3 of 3 matches:\n",
      " plays . widespread gains in energy stocks are inflating the portfolios of agg\n",
      "st levels of the year , with energy stocks outperforming all other market sect\n",
      "utions that sma | | and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "scovering value in natural resource stocks elgin resources ( elr - tsx ) extra\n",
      "Displaying 2 of 2 matches:\n",
      " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
      "one trade tuesday ! go mogi . penny stocks are considered highly speculative a\n",
      "Displaying 5 of 5 matches:\n",
      "hursday ! some of these littie voip stocks have been realiy moving lateiy . an\n",
      "t can happen with these sma | | cap stocks when they take off . and it happens\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 4 of 4 matches:\n",
      " the last 12 months , many of these stocks made triple and even quadruple retu\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 5 of 5 matches:\n",
      "ck monday some of these littie voip stocks have been really moving lately . an\n",
      "t can happen with these sma | | cap stocks when they take off . and it happens\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "ck monday some of these little voip stocks have been realiy moving lately . an\n",
      " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
      "Displaying 3 of 3 matches:\n",
      "5 how many times have you seen good stocks but you couldn ' t get your hands o\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      "k tuesday some of these littie voip stocks have been reaily moving lateiy . an\n",
      " statements . as with many microcap stocks , today ' s company has additional \n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      "ck monday some of these little voip stocks have been rea | | y moving lately .\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 1 of 1 matches:\n",
      "the | ast 12 months , many of these stocks made triple and even quadruple retu\n",
      "Displaying 1 of 1 matches:\n",
      "or information puposes only . penny stocks are considered highly speculative a\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy speculative a\n",
      "Displaying 4 of 4 matches:\n",
      "watch this one trade . these little stocks can surprise in a big way sometimes\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "dge - ksige are you tired of buying stocks and not having them perform ? our s\n",
      "Displaying 3 of 3 matches:\n",
      "report reveals this smallcap rocket stocks newsletter first we would like to s\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "ck monday some of these little voip stocks have been really moving lately . an\n",
      " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
      "Displaying 3 of 3 matches:\n",
      "n how many times have you seen good stocks but you couldn ' t get your hands o\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "ne trade thursday ! go fcdh . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
      "Displaying 2 of 2 matches:\n",
      "subject : penny stocks are about timing nomad internationa\n",
      " one trade friday ! go ndin . penny stocks are considered highiy speculative a\n",
      "Displaying 4 of 4 matches:\n",
      "tion is key to stock success rocket stocks newsletter u r g e n t i n v e s t \n",
      "ht occur . as with many micro - cap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go ndin . penny stocks are considered highly speculative a\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 1 of 1 matches:\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ penny - stocks are considered highly speculative a\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n"
     ]
    }
   ],
   "source": [
    "# create function to check context of specific words\n",
    "\n",
    "def concordance(data_list, search_word):\n",
    "    for email in data_list:\n",
    "        word_list = [word for word in word_tokenize(email.lower())]\n",
    "        text_list = Text(word_list)\n",
    "        if search_word in word_list:\n",
    "            # default prints out 36 characters before and after search_word\n",
    "            text_list.concordance(search_word)\n",
    "\n",
    "print(\"STOCKS in HAM:\")\n",
    "concordance(ham_list, \"stocks\")\n",
    "\n",
    "print()\n",
    "print(\"STOCKS in SPAM:\")\n",
    "concordance(spam_list, \"stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff85279",
   "metadata": {},
   "source": [
    "# Step 6: Run classifier on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e6287",
   "metadata": {},
   "source": [
    "Read in new data (enron2) and apply the trained classifier to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658273d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the contents of the ham files\n",
    "\n",
    "# initialize empty ham_list2\n",
    "ham_list2 = []\n",
    "\n",
    "# use os.walk to access files that have been uploaded to drive\n",
    "for root, dirs, files in os.walk(\"Data/enron2/ham\"):\n",
    "    # iterate through each separate file\n",
    "    for file in files:\n",
    "\n",
    "        # if the file is a text file\n",
    "        if file.endswith('.txt'):\n",
    "\n",
    "            # open the file as f\n",
    "            with open(os.path.join(root, file), 'r', \n",
    "                      encoding = \"ISO-8859-1\", \n",
    "                      errors=\"ignore\") as f:\n",
    "                \n",
    "                # read the file and save as variable text\n",
    "                text = f.read()\n",
    "\n",
    "                # append text to ham_list\n",
    "                ham_list2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d82e307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat step above for spam_list2\n",
    "\n",
    "spam_list2 = []\n",
    "for root, dirs, files in os.walk(\"Data/enron2/spam\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(root, file), 'r', \n",
    "                      encoding = \"ISO-8859-1\", \n",
    "                      errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                spam_list2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd9b11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read both list contents into single list all_emails2, storing email content \n",
    "# and content label (ham or spam) in tuples\n",
    "\n",
    "# create all_emails2 with spam_list2\n",
    "all_emails2 = [(email_content, \"spam\") for email_content in spam_list2]\n",
    "\n",
    "# add ham_list2 to all_emails2\n",
    "all_emails2 += [(email_content, \"ham\") for email_content in ham_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfd88dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples called all_features2\n",
    "# that iterates over all texts (emails) in the all_emails2 list\n",
    "# and adds a tuple for each email\n",
    "# the tuple contains the features dictionary (from get_features) and the \n",
    "# label for each email (ham or spam) (from all_emails2)\n",
    "\n",
    "all_features2 = [(get_features(email), label) for (email, label) in all_emails2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36ab9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.759433156906266\n"
     ]
    }
   ],
   "source": [
    "# accuracy on new test set enron2\n",
    "print(f\"Accuracy on the test set: \\\n",
    "{classify.accuracy(classifier, all_features2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e701895",
   "metadata": {},
   "source": [
    "# Step 7: More data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94269826",
   "metadata": {},
   "source": [
    "Combine both enron1 and enron2 data sets into one set, create a new classifier and train it on 80% of the larger set and test it on 20%. This will show you results from a classifier that has trained on a lot more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fa8a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the contents of the spam files\n",
    "\n",
    "# initialize empty spam_list_all\n",
    "spam_list_all = []\n",
    "\n",
    "# use os.walk to access files that have been uploaded to drive\n",
    "for root, dirs, files in os.walk(\"Data/enron_all/spam\"):\n",
    "    \n",
    "    # iterate through each separate file\n",
    "    for file in files:\n",
    "\n",
    "        # if the file is a text file\n",
    "        if file.endswith('.txt'):\n",
    "\n",
    "            # open the file as f\n",
    "            with open(os.path.join(root, file), 'r', \n",
    "                      encoding = \"ISO-8859-1\", \n",
    "                      errors=\"ignore\") as f:\n",
    "                \n",
    "                # read the file and save as variable text\n",
    "                text = f.read()\n",
    "\n",
    "                # append text to ham_list\n",
    "                spam_list_all.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ddb7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat step above for ham_list_all\n",
    "\n",
    "ham_list_all = []\n",
    "for root, dirs, files in os.walk(\"Data/enron_all/ham\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(root, file), 'r', \n",
    "                      encoding = \"ISO-8859-1\", \n",
    "                      errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                ham_list_all.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98698e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read both list contents into single list all_emails3\n",
    "# and content label (ham or spam) in tuples\n",
    "\n",
    "# create all_emails2 with spam_list2\n",
    "all_emails3 = [(email_content, \"spam\") for email_content in spam_list_all]\n",
    "\n",
    "# add ham_list2 to all_emails2\n",
    "all_emails3 += [(email_content, \"ham\") for email_content in ham_list_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "153d5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples called all_features3\n",
    "# that iterates over all texts (emails) in the all_emails3 list\n",
    "# and adds a tuple for each email\n",
    "# the tuple contains the features dictionary (from get_features) and the \n",
    "# label for each email (ham or spam) (from all_emails3)\n",
    "\n",
    "all_features3 = [(get_features(email), label) \n",
    "                 for (email, label) \n",
    "                 in all_emails3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bc00496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7015 emails\n",
      "Test set size: 1754 emails\n"
     ]
    }
   ],
   "source": [
    "# apply the train function using 80% of emails for training\n",
    "train_set3, test_set3, classifier3 = train(all_features3, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8264abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.9786748774090547\n"
     ]
    }
   ],
   "source": [
    "# accuracy on new test set enron_all\n",
    "print(f\"Accuracy on the test set: \\\n",
    "{classify.accuracy(classifier3, all_features3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
