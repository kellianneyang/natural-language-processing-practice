{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca97de7",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer Using Sentiment Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036ddd7",
   "metadata": {},
   "source": [
    "From Chapter 7 of Getting Started with Natural Language Processing (2022, Kochmar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84249d45",
   "metadata": {},
   "source": [
    "## 0. Understanding your task\n",
    "\n",
    "Suppose you are planning an evening out with some friends, and you'd like to go to a cinema. Your friends' preferences seem to have divided between a superhero movie and an action movie. Both start around the same time, and you like both genres. To choose which group of friends to join at the cinema, you decide to check what those who have already seen these movies think about them. You visit a movie review website and find out that there are hundreds of reviews about both movies. Reading through all these reviews would not be feasible, so you decide to apply a sentiment analyzer to see how many positive and negative opinions there are about each of these movies and then make up your mind. How can you implement such a sentiment analyzer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f2743",
   "metadata": {},
   "source": [
    "## 1. Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79778351",
   "metadata": {},
   "source": [
    "Take a collection of positive and negative reviews. Set up a machine-learning pipeline, as you did for the applications in the previous chapters. This pipeline should rely on the dataset of reviews previously determined to be positive and negative. You should split this set into training and test data, define the set of features to learn the sentiment from, train a classifier of your choice on the training data with the selected set of features, and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b6ed7",
   "metadata": {},
   "source": [
    "## 2. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151630c2",
   "metadata": {},
   "source": [
    "Data: polarity dataset v2.0 from http://mng.bz/Wxnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784f119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, codecs\n",
    "\n",
    "# helper function to read in data\n",
    "def read_in(folder):\n",
    "    files = os.listdir(folder)\n",
    "    a_dict = {}\n",
    "    for a_file in sorted(files):\n",
    "        if not a_file.startswith(\".\"):\n",
    "            with codecs.open(folder + a_file,\n",
    "                            encoding = 'ISO-8859-1',\n",
    "                            errors = 'ignore') as f:\n",
    "                file_id = a_file.split(\".\")[0].strip()\n",
    "                a_dict[file_id] = f.read()\n",
    "            f.close()\n",
    "    return a_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60688594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 1000\n",
      "First positive review:\n",
      "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
      "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
      "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \n",
      "the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \n",
      "in other words , don't dismiss this film because of its source . \n",
      "if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes . \n",
      "getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that's set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \n",
      "the ghetto in question is , of course , whitechapel in 1888 london's east end . \n",
      "it's a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \n",
      "when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \n",
      "abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \n",
      "upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn't so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can't stomach . \n",
      "i don't think anyone needs to be briefed on jack the ripper , so i won't go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \n",
      "in the comic , they don't bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \n",
      "it's funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \n",
      "and from hell's ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \n",
      "don't worry - it'll all make sense when you see it . \n",
      "now onto from hell's appearance : it's certainly dark and bleak enough , and it's surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \n",
      "the print i saw wasn't completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don't say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \n",
      "oscar winner martin childs' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \n",
      "even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \n",
      "ians holm ( joe gould's secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \n",
      "i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn't half bad . \n",
      "the film , however , is all good . \n",
      "2 : 00 - r for strong violence/gore , sexuality , language and drug content \n",
      "\n",
      "\n",
      "Number of negative reviews: 1000\n",
      "First negative review:\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify folder\n",
    "folder = \"Data/txt_sentoken/\"\n",
    "\n",
    "# read positive reviews into a dict, print len and first entry\n",
    "pos_dict = read_in(folder + \"pos/\")\n",
    "\n",
    "# check\n",
    "print(f\"Number of positive reviews: {len(pos_dict)}\")\n",
    "print(\"First positive review:\")\n",
    "print(pos_dict.get(next(iter(pos_dict))))\n",
    "\n",
    "print()\n",
    "\n",
    "# same for negative reviews\n",
    "neg_dict = read_in(folder + \"neg/\")\n",
    "\n",
    "# check\n",
    "print(f\"Number of negative reviews: {len(neg_dict)}\")\n",
    "print(\"First negative review:\")\n",
    "print(neg_dict.get(next(iter(neg_dict))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c41d7b",
   "metadata": {},
   "source": [
    "Reviews are now stored in two python dictionaries (pos_dict and neg_dict), each 1000 items long. Each item has a unique identifier key and the text of the review as the value.\n",
    "\n",
    "The reviews themselves are each a string, where each new sentence in the string is on a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b035b3",
   "metadata": {},
   "source": [
    "## 3. Quantitative checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee0c0b",
   "metadata": {},
   "source": [
    "Texts are already tokenized, so to extract words, the texts just need to be split on whitespaces.\n",
    "\n",
    "Calculate:\n",
    "\n",
    "   1. Average length of a review (in words)\n",
    "   2. Average sentence length\n",
    "   3. Vocabulary size (number of distinct words/word types)\n",
    "   4. Lexical diversity (type-token ratio; measures how often, on average, each word occurs; if each word were unique, measure would equal 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f433328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to split reviews by whitespaces\n",
    "def tokenize(text):\n",
    "    \n",
    "    # replace new lines with spaces\n",
    "    text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a7b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(a_dict):\n",
    "    \n",
    "    # initialize variables\n",
    "    length = 0\n",
    "    sent_length = 0\n",
    "    num_sents = 0\n",
    "    vocab = []\n",
    "    \n",
    "    # iterate over values in dict\n",
    "    for review in a_dict.values():\n",
    "        \n",
    "        # add number of words in review to length\n",
    "        length += len(tokenize(review))\n",
    "        \n",
    "        # to get number of sentences, split review on new lines\n",
    "        sents = review.split(\"\\n\")\n",
    "        num_sents += len(sents)\n",
    "        \n",
    "        # count sentence length\n",
    "        for sent in sents:\n",
    "            sent_length += len(tokenize(sent))\n",
    "            \n",
    "        # add word to vocab\n",
    "        vocab += tokenize(review)\n",
    "    \n",
    "    # average length of a review (in words)\n",
    "    avg_length = float(length) / len(a_dict)\n",
    "    \n",
    "    # average length of sentence\n",
    "    avg_sent_length = float(sent_length) / num_sents\n",
    "    \n",
    "    # vocab size\n",
    "    vocab_size = len(set(vocab))\n",
    "    \n",
    "    # diversity\n",
    "    diversity = float(length) / float(vocab_size)\n",
    "    \n",
    "    return avg_length, avg_sent_length, vocab_size, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d951be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Category  Avg_Len(Review)  Avg_Len(Sent)  Vocabulary Size  Diversity \n",
      " Positive  787.051000       23.191531      36805.000000     21.384350 \n",
      " Negative  705.630000       21.524266      34542.000000     20.428174 \n"
     ]
    }
   ],
   "source": [
    "# print statistics in tabular format\n",
    "\n",
    "categories = [\"Positive\", \"Negative\"]\n",
    "rows = []\n",
    "rows.append([\"Category\", \"Avg_Len(Review)\", \"Avg_Len(Sent)\", \"Vocabulary Size\", \"Diversity\"])\n",
    "\n",
    "stats = {}\n",
    "stats[\"Positive\"] = statistics(pos_dict)\n",
    "stats[\"Negative\"] = statistics(neg_dict)\n",
    "\n",
    "for cat in categories:\n",
    "    rows.append([cat, \n",
    "                 f\"{stats.get(cat)[0]:.6f}\",\n",
    "                 f\"{stats.get(cat)[1]:.6f}\",\n",
    "                 f\"{stats.get(cat)[2]:.6f}\",\n",
    "                 f\"{stats.get(cat)[3]:.6f}\"])\n",
    "\n",
    "columns = zip(*rows)\n",
    "column_widths = [max(len(item) for item in col) for col in columns]\n",
    "\n",
    "for row in rows:\n",
    "    print(''.join(' {:{width}} '.format(row[i], \n",
    "                                        width = column_widths[i]) \n",
    "                                        for i in range(0, len(row))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2ec9d",
   "metadata": {},
   "source": [
    "These statistics show that positive reviews, on average, are longer overall (in number of words), and have longer sentences (in number of words) than negative reviews. They also have a larger vocabulary size. Perhaps partially due to the longer length, positive reviews repeat the same word on average about 21 times, while negative reviews repeat the same word on average about 20 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980ee7d",
   "metadata": {},
   "source": [
    "Are there differences between the two sets of vocabulary used for positive and negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47bb9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_difference(list1, list2):\n",
    "    \n",
    "    # initialize vocabulary variable for each dict\n",
    "    vocab1 = []\n",
    "    vocab2 = []\n",
    "    \n",
    "    # create vocabulary of tokenized reviews for each dict\n",
    "    for rev in list1:\n",
    "        vocab1 += tokenize(rev)\n",
    "        \n",
    "    for rev in list2:\n",
    "        vocab2 += tokenize(rev)\n",
    "\n",
    "    # return the a sorted list of the set (only one instance of each word)\n",
    "    # of the vocabulary difference between the two lists\n",
    "    return sorted(list(set(vocab1) - set(vocab2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eff1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lists for each dict\n",
    "pos_wordlist = pos_dict.values()\n",
    "neg_wordlist = neg_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "972aab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in pos_dict only: 16378\n",
      "Number of unique words in neg_dict only: 14115\n"
     ]
    }
   ],
   "source": [
    "# check size of unique vocabularies\n",
    "print(\"Number of unique words in pos_dict only: \" \\\n",
    "      f\"{str(len(vocab_difference(pos_wordlist, neg_wordlist)))}\")\n",
    "\n",
    "print(\"Number of unique words in neg_dict only: \" \\\n",
    "      f\"{str(len(vocab_difference(neg_wordlist, pos_wordlist)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb615c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words unique to pos_dict:\n",
      "['agreeably', 'agreements', 'agricultural', 'aguilar', 'aguirresarobe', 'ah-ha', 'ahab', 'ahmad', \"ahmad's\", 'aides', 'aids-afflicted', 'aids-cautionary', 'ailment', \"ain't-i-sexy\", 'airborne', 'airbrushed', 'airplane-type', 'airplay', 'airs', 'airtime', 'airwaves', 'aisling', 'aix', 'ajay', 'akiko', \"al'\", 'alabama', 'alacrity', 'aladdin', \"alain's\", 'alakina', 'alamo', 'alanis', 'alar', 'albania', 'albanian', 'albany', 'albarn', 'alberta', 'albums', 'alc', 'alchemy', 'alcohol-based', \"alda's\", 'alderaan', 'aldous', 'alea', 'alejandro', 'alek', 'alexa', \"alexander's\", 'alexandra', 'alexandria', 'alexi-malle', 'alferd', 'alfredo', 'algar', 'algeria', \"ali's\", 'aliases', 'alibi', 'alibis', 'alida', \"alien's\", 'alien-like', 'alien3', 'alienating', 'aliens-as-human', 'aligned', 'aline', 'all--except', 'all-ages', 'all-america', 'all-but-dead', 'all-but-illegible', 'all-consuming', 'all-day', 'all-grown-up', 'all-in-all', 'all-male', 'all-or-nothing', 'all-powerful', 'all-purple', 'all-seeing', 'all-stops-out', 'all-terrain', 'all-together', 'all-too-real', 'all-too-realistic', 'all-too-true', 'all-too-willing', 'all-white', 'allah', 'allegations', 'allegiances', 'allegorical', 'allegra', 'alleviated', 'alleviates', 'alleyways']\n",
      "\n",
      "Words unique to neg_dict:\n",
      "[\"all's\", 'all--and', 'all--but', 'all-business', 'all-but-screaming', 'all-girl', 'all-girls', 'all-important', \"all-in-a-day's-work\", 'all-night', 'all-of-a-sudden-superhuman', 'all-rookie', 'all-round', 'all-too-brief', 'all-too-familiar', 'allayah', \"allayah's\", 'allegiance', 'allegorically', 'allen-', 'allergic', 'allergies', 'allergy', 'allergy-prone', 'allman', 'allocated', 'allotted', 'allotting', 'allred', 'alludes', 'allyson', \"alma's\", 'almasy', 'almost-as-stunning', 'almost-halfway-there', 'almost-subliminal', 'almost-witty', \"alone'\", 'alone_', 'along--and', 'aloofness', 'alpha', 'alphabet', 'already-thin', 'also-ran', 'also-rans', 'alt-rock', 'altercations', 'alterior', 'alternated', 'altman-esque', 'altmanesque', 'altruist', 'altruistic', 'aluminium', 'alums', 'alway', 'always-', 'always-broke', 'always-likable', 'always-woozy', \"alzhiemer's\", 'amalgamation', \"amanda's\", 'amarillo', 'amateurish-foregrounds', 'amateurishly', 'amateurism', 'amazonas', 'ambience', 'ambitionless', 'ambivlaent', 'amblyn', \"amblyn's\", 'amboy', 'amboys', 'ambushes', 'amendments', \"american'\", 'american-ized', 'americana', 'americanization', 'americanizing', 'ami', 'amicable', 'amiel', 'amigos', 'amis', 'amish', 'amish-mocking', 'ammo', 'ammunitions', 'amoeba', 'amor', 'amorous', 'amounting', 'amourous', 'amphetimenes', 'amphibian-based', 'amply-sized']\n"
     ]
    }
   ],
   "source": [
    "# check unique words\n",
    "\n",
    "print(\"Words unique to pos_dict:\")\n",
    "print(vocab_difference(pos_wordlist, neg_wordlist)[1000:1100])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Words unique to neg_dict:\")\n",
    "print(vocab_difference(neg_wordlist, pos_wordlist)[1000:1100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e74c8",
   "metadata": {},
   "source": [
    "## 4. Preprocessing with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11d6c2",
   "metadata": {},
   "source": [
    "Extract linguistic info such as lemmas and POS tags with spaCy's pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e16cfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fcadbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_preprocess_reviews(source):\n",
    "    \n",
    "    # create container dictionary to hold each review's ling info\n",
    "    source_docs = {}\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    # iterate through dict items\n",
    "    for review_id in source.keys():\n",
    "        \n",
    "        # add ling info for each review to source_docs\n",
    "        # get review text by looking up review_id\n",
    "        # convert each review into one line of text (take out new lines)\n",
    "        # disable named entity recognition module to speed up processing\n",
    "        source_docs[review_id] = nlp(source.get(review_id).replace(\"\\n\", \"\"), \n",
    "                                     disable = [\"ner\"])\n",
    "        \n",
    "        # print progress statement every 200 reviews\n",
    "        if index > 0 and (index % 200) == 0:\n",
    "            print(str(index) + \" reviews processed\")\n",
    "        \n",
    "        # increment index\n",
    "        index += 1\n",
    "    \n",
    "    print(\"Dataset processed\")\n",
    "    \n",
    "    return source_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "522323ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 reviews processed\n",
      "400 reviews processed\n",
      "600 reviews processed\n",
      "800 reviews processed\n",
      "Dataset processed\n",
      "200 reviews processed\n",
      "400 reviews processed\n",
      "600 reviews processed\n",
      "800 reviews processed\n",
      "Dataset processed\n"
     ]
    }
   ],
   "source": [
    "pos_docs = spacy_preprocess_reviews(pos_dict)\n",
    "neg_docs = spacy_preprocess_reviews(neg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93badd34",
   "metadata": {},
   "source": [
    "## 5. Quantitative checks (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d600a",
   "metadata": {},
   "source": [
    "Now that we have linguistic information on the reviews from spaCy, we can calculate statistics on the reviews in a more targeted manner. Here we revise the quantitative checks from section 3 above to calculate statistics on the positive and negative dictionaries using lemmas instead of as-is words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef605cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change parameter to source_docs instead of dictionary\n",
    "# so we can access lemmas\n",
    "def statistics_lem(source_docs):\n",
    "    \n",
    "    # initialize variables\n",
    "    length = 0\n",
    "    vocab = []\n",
    "    \n",
    "    # iterate over review_ids in source_docs\n",
    "    for review_id in source_docs.keys():\n",
    "        \n",
    "        # get the text of the review\n",
    "        review_doc = source_docs.get(review_id)\n",
    "        \n",
    "        # initialize list to store lemmas\n",
    "        lemmas = []\n",
    "        \n",
    "        # iterate over words in review_doc\n",
    "        for token in review_doc:\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "        length += len(lemmas)\n",
    "        vocab += lemmas\n",
    "            \n",
    "    \n",
    "    # average length of a review (in words)\n",
    "    avg_length = float(length) / len(source_docs)\n",
    "    \n",
    "    # vocab size\n",
    "    vocab_size = len(set(vocab))\n",
    "    \n",
    "    # diversity\n",
    "    diversity = float(length) / float(vocab_size)\n",
    "    \n",
    "    return avg_length, vocab_size, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e9869bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Category  Avg_Len(Review)  Vocabulary Size  Diversity \n",
      " Positive  818.959000       24487.000000     33.444644 \n",
      " Negative  737.666000       22872.000000     32.251924 \n"
     ]
    }
   ],
   "source": [
    "# print statistics in tabular format\n",
    "\n",
    "categories = [\"Positive\", \"Negative\"]\n",
    "rows = []\n",
    "rows.append([\"Category\", \"Avg_Len(Review)\",\"Vocabulary Size\", \"Diversity\"])\n",
    "\n",
    "stats = {}\n",
    "stats[\"Positive\"] = statistics_lem(pos_docs)\n",
    "stats[\"Negative\"] = statistics_lem(neg_docs)\n",
    "\n",
    "for cat in categories:\n",
    "    rows.append([cat, \n",
    "                 f\"{stats.get(cat)[0]:.6f}\",\n",
    "                 f\"{stats.get(cat)[1]:.6f}\",\n",
    "                 f\"{stats.get(cat)[2]:.6f}\"])\n",
    "\n",
    "columns = zip(*rows)\n",
    "column_widths = [max(len(item) for item in col) for col in columns]\n",
    "\n",
    "for row in rows:\n",
    "    print(''.join(' {:{width}} '.format(row[i], \n",
    "                                        width = column_widths[i]) \n",
    "                                        for i in range(0, len(row))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23443c8c",
   "metadata": {},
   "source": [
    "Using spaCy to tokenize the text properly, the average length of reviews (in lemmas) went up and the vocabulary size (in lemmas) went down. Diversity went up accordingly, since multiple words use the same lemmas and so they get repeated more often. Overall, the ratio of statistics between positive and negative reviews didn't change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f0c4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_lem_difference(source_docs1, source_docs2):\n",
    "    \n",
    "    # initialize vocabulary variable for each dict\n",
    "    vocab1 = []\n",
    "    vocab2 = []\n",
    "    \n",
    "    # create vocabulary of tokenized reviews for each dict\n",
    "    for rev_id in source_docs1.keys():\n",
    "        rev = source_docs1.get(rev_id)\n",
    "        for token in rev:\n",
    "            vocab1.append(token.lemma_)\n",
    "        \n",
    "    for rev_id in source_docs2.keys():\n",
    "        rev = source_docs2.get(rev_id)\n",
    "        for token in rev:\n",
    "            vocab2.append(token.lemma_)\n",
    "\n",
    "    # return the a sorted list of the set (only one instance of each word)\n",
    "    # of the vocabulary difference between the two lists\n",
    "    return sorted(list(set(vocab1) - set(vocab2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86eea6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas in positive reviews only: 9266\n",
      "Number of unique lemmas in negative reviews only: 7651\n"
     ]
    }
   ],
   "source": [
    "# check size of unique lemmas\n",
    "print(\"Number of unique lemmas in positive reviews only: \" \\\n",
    "      f\"{str(len(vocab_lem_difference(pos_docs, neg_docs)))}\")\n",
    "\n",
    "print(\"Number of unique lemmas in negative reviews only: \" \\\n",
    "      f\"{str(len(vocab_lem_difference(neg_docs, pos_docs)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "385cd2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas unique to positive reviews:\n",
      "['agreeably', 'agreements', 'agricultural', 'aguilar', 'aguirresarobe', 'ah-ha', 'ahab', 'ahmad', \"ahmad's\", 'aides', 'aids-afflicted', 'aids-cautionary', 'ailment', \"ain't-i-sexy\", 'airborne', 'airbrushed', 'airplane-type', 'airplay', 'airs', 'airtime', 'airwaves', 'aisling', 'aix', 'ajay', 'akiko', \"al'\", 'alabama', 'alacrity', 'aladdin', \"alain's\", 'alakina', 'alamo', 'alanis', 'alar', 'albania', 'albanian', 'albany', 'albarn', 'alberta', 'albums', 'alc', 'alchemy', 'alcohol-based', \"alda's\", 'alderaan', 'aldous', 'alea', 'alejandro', 'alek', 'alexa', \"alexander's\", 'alexandra', 'alexandria', 'alexi-malle', 'alferd', 'alfredo', 'algar', 'algeria', \"ali's\", 'aliases', 'alibi', 'alibis', 'alida', \"alien's\", 'alien-like', 'alien3', 'alienating', 'aliens-as-human', 'aligned', 'aline', 'all--except', 'all-ages', 'all-america', 'all-but-dead', 'all-but-illegible', 'all-consuming', 'all-day', 'all-grown-up', 'all-in-all', 'all-male', 'all-or-nothing', 'all-powerful', 'all-purple', 'all-seeing', 'all-stops-out', 'all-terrain', 'all-together', 'all-too-real', 'all-too-realistic', 'all-too-true', 'all-too-willing', 'all-white', 'allah', 'allegations', 'allegiances', 'allegorical', 'allegra', 'alleviated', 'alleviates', 'alleyways']\n",
      "\n",
      "Lemmas unique to negative_reviews:\n",
      "[\"all's\", 'all--and', 'all--but', 'all-business', 'all-but-screaming', 'all-girl', 'all-girls', 'all-important', \"all-in-a-day's-work\", 'all-night', 'all-of-a-sudden-superhuman', 'all-rookie', 'all-round', 'all-too-brief', 'all-too-familiar', 'allayah', \"allayah's\", 'allegiance', 'allegorically', 'allen-', 'allergic', 'allergies', 'allergy', 'allergy-prone', 'allman', 'allocated', 'allotted', 'allotting', 'allred', 'alludes', 'allyson', \"alma's\", 'almasy', 'almost-as-stunning', 'almost-halfway-there', 'almost-subliminal', 'almost-witty', \"alone'\", 'alone_', 'along--and', 'aloofness', 'alpha', 'alphabet', 'already-thin', 'also-ran', 'also-rans', 'alt-rock', 'altercations', 'alterior', 'alternated', 'altman-esque', 'altmanesque', 'altruist', 'altruistic', 'aluminium', 'alums', 'alway', 'always-', 'always-broke', 'always-likable', 'always-woozy', \"alzhiemer's\", 'amalgamation', \"amanda's\", 'amarillo', 'amateurish-foregrounds', 'amateurishly', 'amateurism', 'amazonas', 'ambience', 'ambitionless', 'ambivlaent', 'amblyn', \"amblyn's\", 'amboy', 'amboys', 'ambushes', 'amendments', \"american'\", 'american-ized', 'americana', 'americanization', 'americanizing', 'ami', 'amicable', 'amiel', 'amigos', 'amis', 'amish', 'amish-mocking', 'ammo', 'ammunitions', 'amoeba', 'amor', 'amorous', 'amounting', 'amourous', 'amphetimenes', 'amphibian-based', 'amply-sized']\n"
     ]
    }
   ],
   "source": [
    "# check unique lemmas\n",
    "\n",
    "print(\"Lemmas unique to positive reviews:\")\n",
    "print(vocab_lem_difference(pos_docs, neg_docs)[1000:1100])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Lemmas unique to negative_reviews:\")\n",
    "print(vocab_lem_difference(neg_docs, pos_docs)[1000:1100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d26d19",
   "metadata": {},
   "source": [
    "Here we will add in code to check specifically the unique adjectives and adverbs for the two types of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d8e4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise functions to act on adjectives and adverbs\n",
    "def statistics_ad(source_docs):\n",
    "    \n",
    "    # initialize variables\n",
    "    length = 0\n",
    "    vocab = []\n",
    "    \n",
    "    # iterate over review_ids in source_docs\n",
    "    for review_id in source_docs.keys():\n",
    "        \n",
    "        # get the text of the review\n",
    "        review_doc = source_docs.get(review_id)\n",
    "        \n",
    "        # initialize list to store lemmas\n",
    "        ad_lemmas = []\n",
    "        \n",
    "        # iterate over words in review_doc\n",
    "        for token in review_doc:\n",
    "            \n",
    "            if token.pos_ == 'ADJ' or token.pos_ == 'ADV':\n",
    "                ad_lemmas.append(token.lemma_)\n",
    "            \n",
    "        length += len(ad_lemmas)\n",
    "        vocab += ad_lemmas  \n",
    "    \n",
    "    # average length of a review (in words)\n",
    "    avg_num_ad = float(length) / len(source_docs)\n",
    "    \n",
    "    # vocab size\n",
    "    total_num_ad = len(set(vocab))\n",
    "    \n",
    "    # diversity\n",
    "    diversity_ad = float(length) / float(total_num_ad)\n",
    "    \n",
    "    return avg_num_ad, total_num_ad, diversity_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adce8a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Category  Avg Number of ADJ and ADV per Review  Total Number of ADJ and ADV (Vocab)  Diversity \n",
      " Positive  107.159000                            6791.000000                          15.779561 \n",
      " Negative  94.703000                             6303.000000                          15.025067 \n"
     ]
    }
   ],
   "source": [
    "# print statistics in tabular format\n",
    "\n",
    "categories = [\"Positive\", \"Negative\"]\n",
    "rows = []\n",
    "rows.append([\"Category\", \n",
    "             \"Avg Number of ADJ and ADV per Review\", \n",
    "             \"Total Number of ADJ and ADV (Vocab)\", \n",
    "             \"Diversity\"])\n",
    "\n",
    "stats = {}\n",
    "stats[\"Positive\"] = statistics_ad(pos_docs)\n",
    "stats[\"Negative\"] = statistics_ad(neg_docs)\n",
    "\n",
    "for cat in categories:\n",
    "    rows.append([cat, \n",
    "                 f\"{stats.get(cat)[0]:.6f}\",\n",
    "                 f\"{stats.get(cat)[1]:.6f}\",\n",
    "                 f\"{stats.get(cat)[2]:.6f}\"])\n",
    "\n",
    "columns = zip(*rows)\n",
    "column_widths = [max(len(item) for item in col) for col in columns]\n",
    "\n",
    "for row in rows:\n",
    "    print(''.join(' {:{width}} '.format(row[i], \n",
    "                                        width = column_widths[i]) \n",
    "                                        for i in range(0, len(row))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2ea2947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_adj_difference(source_docs1, source_docs2):\n",
    "    \n",
    "    # initialize vocabulary variable for each dict\n",
    "    vocab1_adj = []\n",
    "    vocab2_adj = []\n",
    "    \n",
    "    # create vocabulary of tokenized reviews for each dict\n",
    "    for rev_id in source_docs1.keys():\n",
    "        rev = source_docs1.get(rev_id)\n",
    "        for token in rev:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                vocab1_adj.append(token.lemma_)\n",
    "        \n",
    "    for rev_id in source_docs2.keys():\n",
    "        rev = source_docs2.get(rev_id)\n",
    "        for token in rev:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                vocab2_adj.append(token.lemma_)\n",
    "\n",
    "    # return the a sorted list of the set (only one instance of each word)\n",
    "    # of the vocabulary difference between the two lists\n",
    "    return sorted(list(set(vocab1_adj) - set(vocab2_adj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59f2711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique adjectives in positive reviews only: 2145\n",
      "Number of unique adjectives in negative reviews only: 1710\n"
     ]
    }
   ],
   "source": [
    "# check size of unique adjectives\n",
    "print(\"Number of unique adjectives in positive reviews only: \" \\\n",
    "      f\"{str(len(vocab_adj_difference(pos_docs, neg_docs)))}\")\n",
    "\n",
    "print(\"Number of unique adjectives in negative reviews only: \" \\\n",
    "      f\"{str(len(vocab_adj_difference(neg_docs, pos_docs)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d6c282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectives unique to positive reviews:\n",
      "['jerome', 'jobless', 'jogs', 'joint', 'joisey', 'jolly', 'joyful', 'joyous', 'judgemental', 'judicious', 'juggling', 'jugular', 'juliani', 'julianne', 'jumble', 'juni', 'just', 'kaleidoscopic', 'kennedyesque', 'kevlar', 'kieslowski', 'kilar', 'kimble', 'kindest', 'kindhearted', 'kindly', 'kindred', 'kissed', 'kitchy', 'kitschi', 'klass', 'knowing', 'kotter', 'kotto', 'kozmo', 'kyzynski', 'labyrinthine', 'laconic', 'lamenting', 'larious', 'laszlo', 'laughs', 'lauri', 'lawman', 'lawyerly', 'lecherous', 'leftist', 'leftover', 'leghorn', 'legible', 'lemon', 'lensing', 'lent', 'lester', 'level', 'liberating', 'libidinous', 'licentious', 'lightheaded', 'lightyear', 'lighweight', 'lillian', 'line', 'lionized', 'living', 'longish', 'longstanding', 'loomis', 'loony', 'loopy', 'lopez', 'lopsided', 'losin', 'loudmouthed', 'loved', 'lovelorn', 'lowe', 'loyalist', 'lucid', 'lumpy', 'lust', 'lynchian', 'm', 'machiavellian', 'magisterial', 'magnificient', 'maiden', 'maine', 'malachy', 'malcovich', 'mallory', 'malnourished', 'maltese', 'manageable', 'mandated', 'manhatten', 'maniacial', 'manifest', 'manifested', 'manifold']\n",
      "\n",
      "Adjectives unique to negative_reviews:\n",
      "['opportune', 'optical', 'optional', 'orangey', 'orangutan', 'orgasmic', 'orginal', 'orientated', 'orignal', 'ornate', 'orphaned', 'oscar-', 'other--', 'outward', 'overabundant', 'overal', 'overcast', 'overconfident', 'overcooked', 'overcrowded', 'overdramaticized', 'overeacting', 'overemotional', 'overexaggerant', 'overexcited', 'overextended', 'overpaid', 'overplay', 'overplotted', 'overpraised', 'overpriced', 'overprotective', 'overscored', 'overseas', 'oversentimental', 'overwhelm', 'overworked', 'pacifistic', 'padded', 'palatial', 'pammy', 'panky', 'pansexual', 'pansy', 'pantomine', 'paradoxical', 'paraplegic', 'parasitic', 'parentless', 'parochial', 'parted', 'party', 'passing', 'passionless', 'pasty', 'patented', 'patsy', 'paull', 'peachy', 'pear', 'pecan', 'pecked', 'peculiarly', 'pedagogical', 'pedaled', 'peerless', 'pefect', 'pejorative', 'pelvic', 'penile', 'penis', 'percussive', 'perennial', 'perfected', 'peripheral', 'permanant', 'permissive', 'persnickety', 'personable', 'petrified', 'phantastic', 'pharmacological', 'photocopied', 'picture', 'piercing', 'pimp', 'pine', 'pisspoor', 'pithy', 'pitt', 'pleasured', 'plex', 'plissken', 'plods', 'plotless', 'polaroid', 'political-', 'pooh', 'populist', 'populous']\n"
     ]
    }
   ],
   "source": [
    "# check unique adjectives\n",
    "\n",
    "print(\"Adjectives unique to positive reviews:\")\n",
    "print(vocab_adj_difference(pos_docs, neg_docs)[1000:1100])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Adjectives unique to negative_reviews:\")\n",
    "print(vocab_adj_difference(neg_docs, pos_docs)[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0c727ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_adv_difference(source_docs1, source_docs2):\n",
    "    \n",
    "    # initialize vocabulary variable for each dict\n",
    "    vocab1_adv = []\n",
    "    vocab2_adv = []\n",
    "    \n",
    "    # create vocabulary of tokenized reviews for each dict\n",
    "    for rev_id in source_docs1.keys():\n",
    "        rev = source_docs1.get(rev_id)\n",
    "        for token in rev:\n",
    "            if token.pos_ == 'ADV':\n",
    "                vocab1_adv.append(token.lemma_)\n",
    "        \n",
    "    for rev_id in source_docs2.keys():\n",
    "        rev = source_docs2.get(rev_id)\n",
    "        for token in rev:\n",
    "            if token.pos_ == 'ADV':\n",
    "                vocab2_adv.append(token.lemma_)\n",
    "\n",
    "    # return the a sorted list of the set (only one instance of each word)\n",
    "    # of the vocabulary difference between the two lists\n",
    "    return sorted(list(set(vocab1_adv) - set(vocab2_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ef25a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique adverbs in positive reviews only: 568\n",
      "Number of unique adverbs in negative reviews only: 477\n"
     ]
    }
   ],
   "source": [
    "# check size of unique adverbs\n",
    "print(\"Number of unique adverbs in positive reviews only: \" \\\n",
    "      f\"{str(len(vocab_adv_difference(pos_docs, neg_docs)))}\")\n",
    "\n",
    "print(\"Number of unique adverbs in negative reviews only: \" \\\n",
    "      f\"{str(len(vocab_adv_difference(neg_docs, pos_docs)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "041e8690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adverbs unique to positive reviews:\n",
      "['fuller', 'garber', 'geek', 'geographically', 'gereally', 'ghostly', 'giddily', 'gladys', 'globally', 'goateed', 'googly', 'graciously', 'gravely', 'grier', 'grim', 'gristly', 'grotesquely', 'groundbreakingly', 'grudgingly', 'gruff', 'guido+s', 'guiltily', 'hank', 'hanly', 'harmlessly', 'harrowingly', 'harsh', 'harshly', 'head', 'henceforth', 'hesitantly', 'hillbilly', 'hipster', 'hollow', 'homily', 'honorably', 'horizontally', 'horrifyingly', 'howard', 'humorless', 'hutter', 'hypernaturally', \"i'm\", 'icily', 'idealistically', 'ideologically', 'illicitly', 'imaginatively', 'immaculately', 'immeasurably', 'immediatly', 'impartially', 'imperiously', 'inadvertly', 'inarguably', 'incompletely', 'indefinantly', 'indelibly', 'indignantly', 'indubitably', 'inelegantly', 'inextricably', 'infrequently', 'infuriatingly', 'ingeniously', 'insidiously', 'insincerely', 'internationally', 'interpol', 'intially', 'intimately', 'intuitively', 'inually', 'irish', 'irishfolk', 'irrevocably', 'it+s', 'jealously', 'jelly', 'jittery', 'jokingly', 'joyfully', 'joyously', 'justly', 'juvenile', 'keely', 'keenly', 'kimberly', 'klingon', 'kronk', 'large', 'latin', 'legally', 'legit', 'legitimately', 'leisurely', 'lesly', 'liberally', 'light', 'limp']\n",
      "\n",
      "Adverbs unique to negative_reviews:\n",
      "['incomprehensibly', 'incongruously', 'inconsistently', 'incronguously', 'incurably', 'indefinitely', 'independece', 'independently', 'indifferently', 'indirectly', 'indulgently', 'ineptly', 'inexcusably', 'initally', 'innocuously', 'innovatively', 'insipidly', 'insultingly', 'intently', 'interminable', 'intermittently', 'involuntarily', 'jarringly', 'jocularly', 'jubilantly', 'justifiably', 'lackadasically', 'laconically', 'lame', 'lamely', 'larter', 'lavishly', 'lazily', 'lead', 'leeringly', 'levritt', 'lifelessly', 'limply', 'locally', 'losely', 'louder', 'lower', 'lowest', 'luridly', 'lustfully', 'lustily', 'macallister', 'madonna', 'majorly', 'maliciously', 'manually', 'mass', 'mayberly', 'mcconnell', 'medically', 'midly', 'mind', 'mindnumbingly', 'minimally', 'minutely', 'miramax', 'mis', 'mockingly', 'momumentally', 'monumentally', 'moralistically', 'musically', 'mystically', 'na', 'narrow', 'nauseatingly', 'neither', 'nether', 'nicky', 'nit', 'non-', 'norm', 'nostalgically', 'nouvelle', \"o'reilly\", 'obscenely', 'occaisionally', 'offensively', 'oliver', 'onwards', 'orgasmically', 'osmet', 'overact', 'overdress', 'overhead', 'overscore', 'overwhelmingly', 'palpably', 'particuarly', 'passingly', 'pathetically', 'pathologically', 'peacefully', 'perfect', 'persevere']\n"
     ]
    }
   ],
   "source": [
    "# check unique adverbs\n",
    "\n",
    "print(\"Adverbs unique to positive reviews:\")\n",
    "print(vocab_adv_difference(pos_docs, neg_docs)[200:300])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Adverbs unique to negative_reviews:\")\n",
    "print(vocab_adv_difference(neg_docs, pos_docs)[200:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfc9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
